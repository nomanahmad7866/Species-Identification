{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# import seaborn as sns\n",
    "# PyTorch\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torch import optim, cuda\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Data science tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Image manipulations\n",
    "from PIL import Image\n",
    "# Useful for examining network\n",
    "from torchsummary import summary\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Printing out all outputs\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = 'models/model2.pth'\n",
    "save_file_name = 'models/model2.pt'\n",
    "# Change to fit hardware\n",
    "batch_size = 64\n",
    "# Whether to train on a gpu\n",
    "train_on_gpu = cuda.is_available()\n",
    "train_on_gpu\n",
    "\n",
    "# Image transformations\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets from each folder\n",
    "data = {\n",
    "    'train':\n",
    "    datasets.ImageFolder(root=  \"data/Train\",      transform=image_transforms['train']),\n",
    "    'valid':\n",
    "    datasets.ImageFolder(root=  \"data/Validation\", transform=image_transforms['valid']),\n",
    "    'test':\n",
    "    datasets.ImageFolder(root=  \"data/Test\",       transform=image_transforms['test'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
    "    'valid': DataLoader(data['valid'], batch_size=batch_size, shuffle=True),\n",
    "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 224, 224]), torch.Size([64]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(data['train'].classes)\n",
    "n_classes\n",
    "trainiter = iter(dataloaders['train'])\n",
    "features, labels = next(trainiter)\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Execute below cell only when you make new model. If you want to load a saved model, then run load_checkpoint method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [64, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [64, 64, 112, 112]             128\n",
      "              ReLU-3         [64, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [64, 64, 56, 56]               0\n",
      "            Conv2d-5           [64, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [64, 64, 56, 56]             128\n",
      "              ReLU-7           [64, 64, 56, 56]               0\n",
      "            Conv2d-8           [64, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [64, 64, 56, 56]             128\n",
      "             ReLU-10           [64, 64, 56, 56]               0\n",
      "       BasicBlock-11           [64, 64, 56, 56]               0\n",
      "           Conv2d-12           [64, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [64, 64, 56, 56]             128\n",
      "             ReLU-14           [64, 64, 56, 56]               0\n",
      "           Conv2d-15           [64, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [64, 64, 56, 56]             128\n",
      "             ReLU-17           [64, 64, 56, 56]               0\n",
      "       BasicBlock-18           [64, 64, 56, 56]               0\n",
      "           Conv2d-19          [64, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [64, 128, 28, 28]             256\n",
      "             ReLU-21          [64, 128, 28, 28]               0\n",
      "           Conv2d-22          [64, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [64, 128, 28, 28]             256\n",
      "           Conv2d-24          [64, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [64, 128, 28, 28]             256\n",
      "             ReLU-26          [64, 128, 28, 28]               0\n",
      "       BasicBlock-27          [64, 128, 28, 28]               0\n",
      "           Conv2d-28          [64, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [64, 128, 28, 28]             256\n",
      "             ReLU-30          [64, 128, 28, 28]               0\n",
      "           Conv2d-31          [64, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [64, 128, 28, 28]             256\n",
      "             ReLU-33          [64, 128, 28, 28]               0\n",
      "       BasicBlock-34          [64, 128, 28, 28]               0\n",
      "           Conv2d-35          [64, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [64, 256, 14, 14]             512\n",
      "             ReLU-37          [64, 256, 14, 14]               0\n",
      "           Conv2d-38          [64, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [64, 256, 14, 14]             512\n",
      "           Conv2d-40          [64, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [64, 256, 14, 14]             512\n",
      "             ReLU-42          [64, 256, 14, 14]               0\n",
      "       BasicBlock-43          [64, 256, 14, 14]               0\n",
      "           Conv2d-44          [64, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [64, 256, 14, 14]             512\n",
      "             ReLU-46          [64, 256, 14, 14]               0\n",
      "           Conv2d-47          [64, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [64, 256, 14, 14]             512\n",
      "             ReLU-49          [64, 256, 14, 14]               0\n",
      "       BasicBlock-50          [64, 256, 14, 14]               0\n",
      "           Conv2d-51            [64, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [64, 512, 7, 7]           1,024\n",
      "             ReLU-53            [64, 512, 7, 7]               0\n",
      "           Conv2d-54            [64, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [64, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [64, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [64, 512, 7, 7]           1,024\n",
      "             ReLU-58            [64, 512, 7, 7]               0\n",
      "       BasicBlock-59            [64, 512, 7, 7]               0\n",
      "           Conv2d-60            [64, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [64, 512, 7, 7]           1,024\n",
      "             ReLU-62            [64, 512, 7, 7]               0\n",
      "           Conv2d-63            [64, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [64, 512, 7, 7]           1,024\n",
      "             ReLU-65            [64, 512, 7, 7]               0\n",
      "       BasicBlock-66            [64, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [64, 512, 1, 1]               0\n",
      "           Linear-68                  [64, 199]         102,087\n",
      "================================================================\n",
      "Total params: 11,278,599\n",
      "Trainable params: 102,087\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 36.75\n",
      "Forward/backward pass size (MB): 4018.35\n",
      "Params size (MB): 43.02\n",
      "Estimated Total Size (MB): 4098.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "#res_model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 199)\n",
    "model.load_state_dict(torch.load('models/model2.pt'))\n",
    "\n",
    "if train_on_gpu:\n",
    "    model = model.to('cuda')\n",
    "\n",
    "summary(model, input_size=(3,224,224), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Compute the topk accuracy(s)\"\"\"\n",
    "    if train_on_gpu:\n",
    "        output = output.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        # Find the predicted classes and transpose\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "\n",
    "        # Determine predictions equal to the targets\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "\n",
    "        # For each k, find the percentage of correct\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Before Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[65.625, 85.9375]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testiter = iter(dataloaders['test'])\n",
    "# Get a batch of testing images and labels\n",
    "features, targets = next(testiter)\n",
    "\n",
    "print('Accuracy Before Training')\n",
    "if train_on_gpu:\n",
    "    accuracy(model(features.to('cuda')), targets, topk=(1, 5))\n",
    "else:\n",
    "    accuracy(model(features), targets, topk=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you load a saved model, it will return an optimizer as well so no need to define here\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0.99, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          criterion,optimizer,train_loader,valid_loader,save_file_name,max_epochs_stop=3,n_epochs=20,print_every=1):\n",
    "\n",
    "    # Early stopping intialization\n",
    "    epochs_no_improve = 0\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    valid_max_acc = 0\n",
    "    history = []\n",
    "\n",
    "    # Number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "    overall_start = timer()\n",
    "\n",
    "    # Main loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # keep track of training and validation loss each epoch\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_acc = 0\n",
    "        valid_acc = 0\n",
    "\n",
    "        # Set to training\n",
    "        model.train()\n",
    "        start = timer()\n",
    "        \n",
    "#         if(epoch >20 and epoch <= 40):\n",
    "#             optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "#         if(epoch >= 1 and epoch <= 20):\n",
    "#             optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "#         if(epoch >20 and epoch <= 40):\n",
    "#             optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
    "#         if(epoch >40 and epoch <= 60):\n",
    "#             optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "        \n",
    "        # Training loop\n",
    "        for ii, (data, target) in enumerate(train_loader):\n",
    "            # Tensors to gpu\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Predicted outputs are log probabilities\n",
    "            output = model(data)\n",
    "\n",
    "            # Loss and backpropagation of gradients\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Calculate accuracy by finding max log probability\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            # Need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            # Multiply average accuracy times the number of examples in batch\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "            # Track training progress\n",
    "            print(\n",
    "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
    "                end='\\r')\n",
    "\n",
    "        # After training loops ends, start validation\n",
    "        else:\n",
    "            model.epochs += 1\n",
    "\n",
    "            # Don't need to keep track of gradients\n",
    "            with torch.no_grad():\n",
    "                # Set to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Validation loop\n",
    "                for data, target in valid_loader:\n",
    "                    # Tensors to gpu\n",
    "                    if train_on_gpu:\n",
    "                        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                    # Forward pass\n",
    "                    output = model(data)\n",
    "\n",
    "                    # Validation loss\n",
    "                    loss = criterion(output, target)\n",
    "                    # Multiply average loss times the number of examples in batch\n",
    "                    valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                    # Calculate validation accuracy\n",
    "                    _, pred = torch.max(output, dim=1)\n",
    "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                    accuracy = torch.mean(\n",
    "                        correct_tensor.type(torch.FloatTensor))\n",
    "                    # Multiply average accuracy times the number of examples\n",
    "                    valid_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "                # Calculate average losses\n",
    "                train_loss = train_loss / len(train_loader.dataset)\n",
    "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "                # Calculate average accuracy\n",
    "                train_acc = train_acc / len(train_loader.dataset)\n",
    "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
    "\n",
    "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
    "\n",
    "                # Print training and validation results\n",
    "                if (epoch + 1) % print_every == 0:\n",
    "                    print(\n",
    "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
    "                    )\n",
    "                    print(\n",
    "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
    "                    )\n",
    "\n",
    "                # Save the model if validation loss decreases\n",
    "                if valid_loss < valid_loss_min:\n",
    "                    # Save model\n",
    "                    torch.save(model.state_dict(), save_file_name)\n",
    "                    # Track improvement\n",
    "                    epochs_no_improve = 0\n",
    "                    valid_loss_min = valid_loss\n",
    "                    valid_best_acc = valid_acc\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                # Otherwise increment count of epochs with no improvement\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    # Trigger early stopping\n",
    "                    if epochs_no_improve >= max_epochs_stop:\n",
    "                        print(\n",
    "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "                        )\n",
    "                        total_time = timer() - overall_start\n",
    "                        print(\n",
    "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
    "                        )\n",
    "\n",
    "                        # Load the best state dict\n",
    "                        model.load_state_dict(torch.load(save_file_name))\n",
    "                        # Attach the optimizer\n",
    "                        model.optimizer = optimizer\n",
    "\n",
    "                        # Format history\n",
    "                        history = pd.DataFrame(\n",
    "                            history,\n",
    "                            columns=[\n",
    "                                'train_loss', 'valid_loss', 'train_acc',\n",
    "                                'valid_acc'\n",
    "                            ])\n",
    "                        return model, history\n",
    "\n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "    # Record overall time and print out stats\n",
    "    total_time = timer() - overall_start\n",
    "    print(\n",
    "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "    )\n",
    "    print(\n",
    "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
    "    )\n",
    "    # Format history\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, path):\n",
    "    # Basic details\n",
    "    \n",
    "    model.class_to_idx = data['train'].class_to_idx\n",
    "    model.idx_to_class = {\n",
    "    idx: class_\n",
    "    for class_, idx in model.class_to_idx.items()\n",
    "    }\n",
    "    checkpoint = {\n",
    "        'class_to_idx': model.class_to_idx,\n",
    "        'idx_to_class': model.idx_to_class,\n",
    "        'epochs': model.epochs,\n",
    "    }\n",
    "\n",
    "    checkpoint['state_dict'] = model.state_dict()\n",
    "    \n",
    "    # Add the optimizer\n",
    "    checkpoint['optimizer'] = model.optimizer\n",
    "    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n",
    "\n",
    "    # Save the data to the path\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path):\n",
    "    # Load in checkpoint\n",
    "    #checkpoint = torch.load(path)\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "        \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    # Make sure to set parameters as not trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 199)\n",
    "    \n",
    "    # Load in the state dict\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'{total_params} total parameters.')\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'{total_trainable_params} total gradient parameters.')\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    # Model basics\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    model.idx_to_class = checkpoint['idx_to_class']\n",
    "    model.epochs = checkpoint['epochs']\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training from Scratch.\n",
      "\n",
      "Epoch: 0\t100.00% complete. 248.44 seconds elapsed in epoch.\n",
      "Epoch: 0 \tTraining Loss: 5.1347 \tValidation Loss: 4.2704\n",
      "\t\tTraining Accuracy: 15.90%\t Validation Accuracy: 28.65%\n",
      "Epoch: 1\t100.00% complete. 85.70 seconds elapsed in epoch.\n",
      "Epoch: 1 \tTraining Loss: 4.1549 \tValidation Loss: 3.8034\n",
      "\t\tTraining Accuracy: 30.17%\t Validation Accuracy: 37.43%\n",
      "Epoch: 2\t100.00% complete. 86.02 seconds elapsed in epoch.\n",
      "Epoch: 2 \tTraining Loss: 3.9344 \tValidation Loss: 3.4916\n",
      "\t\tTraining Accuracy: 34.20%\t Validation Accuracy: 38.68%\n",
      "Epoch: 3\t100.00% complete. 85.52 seconds elapsed in epoch.\n",
      "Epoch: 3 \tTraining Loss: 3.8118 \tValidation Loss: 3.5497\n",
      "\t\tTraining Accuracy: 36.78%\t Validation Accuracy: 40.33%\n",
      "Epoch: 4\t100.00% complete. 85.57 seconds elapsed in epoch.\n",
      "Epoch: 4 \tTraining Loss: 3.5482 \tValidation Loss: 3.7060\n",
      "\t\tTraining Accuracy: 38.71%\t Validation Accuracy: 41.11%\n",
      "Epoch: 5\t100.00% complete. 85.36 seconds elapsed in epoch.\n",
      "Epoch: 5 \tTraining Loss: 3.6089 \tValidation Loss: 3.3440\n",
      "\t\tTraining Accuracy: 39.88%\t Validation Accuracy: 44.18%\n",
      "Epoch: 6\t100.00% complete. 86.19 seconds elapsed in epoch.\n",
      "Epoch: 6 \tTraining Loss: 3.5386 \tValidation Loss: 3.6969\n",
      "\t\tTraining Accuracy: 40.57%\t Validation Accuracy: 43.36%\n",
      "Epoch: 7\t100.00% complete. 85.65 seconds elapsed in epoch.\n",
      "Epoch: 7 \tTraining Loss: 3.5769 \tValidation Loss: 3.8665\n",
      "\t\tTraining Accuracy: 40.96%\t Validation Accuracy: 41.93%\n",
      "Epoch: 8\t100.00% complete. 85.68 seconds elapsed in epoch.\n",
      "Epoch: 8 \tTraining Loss: 3.4086 \tValidation Loss: 3.4213\n",
      "\t\tTraining Accuracy: 41.99%\t Validation Accuracy: 44.35%\n",
      "Epoch: 9\t100.00% complete. 86.28 seconds elapsed in epoch.\n",
      "Epoch: 9 \tTraining Loss: 3.5482 \tValidation Loss: 3.7281\n",
      "\t\tTraining Accuracy: 41.77%\t Validation Accuracy: 41.11%\n",
      "Epoch: 10\t100.00% complete. 86.50 seconds elapsed in epoch.\n",
      "Epoch: 10 \tTraining Loss: 3.4958 \tValidation Loss: 3.5592\n",
      "\t\tTraining Accuracy: 42.31%\t Validation Accuracy: 42.41%\n",
      "Epoch: 11\t100.00% complete. 86.56 seconds elapsed in epoch.\n",
      "Epoch: 11 \tTraining Loss: 3.2590 \tValidation Loss: 3.4377\n",
      "\t\tTraining Accuracy: 43.99%\t Validation Accuracy: 44.31%\n",
      "Epoch: 12\t100.00% complete. 86.69 seconds elapsed in epoch.\n",
      "Epoch: 12 \tTraining Loss: 3.2772 \tValidation Loss: 3.3806\n",
      "\t\tTraining Accuracy: 44.55%\t Validation Accuracy: 43.75%\n",
      "Epoch: 13\t100.00% complete. 86.47 seconds elapsed in epoch.\n",
      "Epoch: 13 \tTraining Loss: 3.3546 \tValidation Loss: 3.8067\n",
      "\t\tTraining Accuracy: 43.83%\t Validation Accuracy: 40.85%\n",
      "Epoch: 14\t100.00% complete. 86.20 seconds elapsed in epoch.\n",
      "Epoch: 14 \tTraining Loss: 3.3678 \tValidation Loss: 3.1964\n",
      "\t\tTraining Accuracy: 42.91%\t Validation Accuracy: 45.22%\n",
      "Epoch: 15\t100.00% complete. 85.93 seconds elapsed in epoch.\n",
      "Epoch: 15 \tTraining Loss: 3.3970 \tValidation Loss: 3.2164\n",
      "\t\tTraining Accuracy: 43.65%\t Validation Accuracy: 45.43%\n",
      "Epoch: 16\t100.00% complete. 86.59 seconds elapsed in epoch.\n",
      "Epoch: 16 \tTraining Loss: 3.2307 \tValidation Loss: 3.6219\n",
      "\t\tTraining Accuracy: 44.64%\t Validation Accuracy: 43.75%\n",
      "Epoch: 17\t100.00% complete. 85.53 seconds elapsed in epoch.\n",
      "Epoch: 17 \tTraining Loss: 3.4622 \tValidation Loss: 3.7401\n",
      "\t\tTraining Accuracy: 43.10%\t Validation Accuracy: 42.58%\n",
      "Epoch: 18\t100.00% complete. 86.38 seconds elapsed in epoch.\n",
      "Epoch: 18 \tTraining Loss: 3.2984 \tValidation Loss: 3.5465\n",
      "\t\tTraining Accuracy: 44.71%\t Validation Accuracy: 43.53%\n",
      "Epoch: 19\t100.00% complete. 85.25 seconds elapsed in epoch.\n",
      "Epoch: 19 \tTraining Loss: 3.2966 \tValidation Loss: 3.5593\n",
      "\t\tTraining Accuracy: 44.18%\t Validation Accuracy: 42.88%\n",
      "Epoch: 20\t100.00% complete. 85.76 seconds elapsed in epoch.\n",
      "Epoch: 20 \tTraining Loss: 3.2683 \tValidation Loss: 3.6045\n",
      "\t\tTraining Accuracy: 45.05%\t Validation Accuracy: 43.49%\n",
      "Epoch: 21\t100.00% complete. 86.33 seconds elapsed in epoch.\n",
      "Epoch: 21 \tTraining Loss: 1.9663 \tValidation Loss: 2.0681\n",
      "\t\tTraining Accuracy: 57.77%\t Validation Accuracy: 54.82%\n",
      "Epoch: 22\t100.00% complete. 86.30 seconds elapsed in epoch.\n",
      "Epoch: 22 \tTraining Loss: 1.7171 \tValidation Loss: 1.9968\n",
      "\t\tTraining Accuracy: 60.11%\t Validation Accuracy: 56.47%\n",
      "Epoch: 23\t100.00% complete. 86.42 seconds elapsed in epoch.\n",
      "Epoch: 23 \tTraining Loss: 1.6602 \tValidation Loss: 1.9224\n",
      "\t\tTraining Accuracy: 61.03%\t Validation Accuracy: 56.86%\n",
      "Epoch: 24\t100.00% complete. 86.92 seconds elapsed in epoch.\n",
      "Epoch: 24 \tTraining Loss: 1.6673 \tValidation Loss: 1.8938\n",
      "\t\tTraining Accuracy: 60.66%\t Validation Accuracy: 57.59%\n",
      "Epoch: 25\t100.00% complete. 86.95 seconds elapsed in epoch.\n",
      "Epoch: 25 \tTraining Loss: 1.6058 \tValidation Loss: 1.8385\n",
      "\t\tTraining Accuracy: 61.99%\t Validation Accuracy: 58.55%\n",
      "Epoch: 26\t100.00% complete. 86.41 seconds elapsed in epoch.\n",
      "Epoch: 26 \tTraining Loss: 1.5551 \tValidation Loss: 1.8192\n",
      "\t\tTraining Accuracy: 63.00%\t Validation Accuracy: 58.63%\n",
      "Epoch: 27\t100.00% complete. 86.35 seconds elapsed in epoch.\n",
      "Epoch: 27 \tTraining Loss: 1.5563 \tValidation Loss: 1.8030\n",
      "\t\tTraining Accuracy: 63.31%\t Validation Accuracy: 58.33%\n",
      "Epoch: 28\t100.00% complete. 86.97 seconds elapsed in epoch.\n",
      "Epoch: 28 \tTraining Loss: 1.5120 \tValidation Loss: 1.7503\n",
      "\t\tTraining Accuracy: 62.30%\t Validation Accuracy: 57.98%\n",
      "Epoch: 29\t100.00% complete. 86.35 seconds elapsed in epoch.\n",
      "Epoch: 29 \tTraining Loss: 1.5295 \tValidation Loss: 1.7947\n",
      "\t\tTraining Accuracy: 62.33%\t Validation Accuracy: 57.46%\n",
      "Epoch: 30\t100.00% complete. 85.84 seconds elapsed in epoch.\n",
      "Epoch: 30 \tTraining Loss: 1.4664 \tValidation Loss: 1.7463\n",
      "\t\tTraining Accuracy: 62.98%\t Validation Accuracy: 58.50%\n",
      "Epoch: 31\t100.00% complete. 86.89 seconds elapsed in epoch.\n",
      "Epoch: 31 \tTraining Loss: 1.4371 \tValidation Loss: 1.7651\n",
      "\t\tTraining Accuracy: 64.57%\t Validation Accuracy: 57.20%\n",
      "Epoch: 32\t100.00% complete. 85.71 seconds elapsed in epoch.\n",
      "Epoch: 32 \tTraining Loss: 1.4724 \tValidation Loss: 1.7092\n",
      "\t\tTraining Accuracy: 63.85%\t Validation Accuracy: 59.11%\n",
      "Epoch: 33\t100.00% complete. 85.76 seconds elapsed in epoch.\n",
      "Epoch: 33 \tTraining Loss: 1.4491 \tValidation Loss: 1.7144\n",
      "\t\tTraining Accuracy: 64.02%\t Validation Accuracy: 58.59%\n",
      "Epoch: 34\t100.00% complete. 86.03 seconds elapsed in epoch.\n",
      "Epoch: 34 \tTraining Loss: 1.4328 \tValidation Loss: 1.6833\n",
      "\t\tTraining Accuracy: 64.37%\t Validation Accuracy: 58.63%\n",
      "Epoch: 35\t100.00% complete. 86.00 seconds elapsed in epoch.\n",
      "Epoch: 35 \tTraining Loss: 1.4089 \tValidation Loss: 1.6843\n",
      "\t\tTraining Accuracy: 64.96%\t Validation Accuracy: 59.15%\n",
      "Epoch: 36\t100.00% complete. 85.57 seconds elapsed in epoch.\n",
      "Epoch: 36 \tTraining Loss: 1.3943 \tValidation Loss: 1.6885\n",
      "\t\tTraining Accuracy: 64.70%\t Validation Accuracy: 59.07%\n",
      "Epoch: 37\t100.00% complete. 85.37 seconds elapsed in epoch.\n",
      "Epoch: 37 \tTraining Loss: 1.4288 \tValidation Loss: 1.6913\n",
      "\t\tTraining Accuracy: 64.18%\t Validation Accuracy: 58.29%\n",
      "Epoch: 38\t100.00% complete. 86.18 seconds elapsed in epoch.\n",
      "Epoch: 38 \tTraining Loss: 1.3878 \tValidation Loss: 1.6617\n",
      "\t\tTraining Accuracy: 65.10%\t Validation Accuracy: 59.24%\n",
      "Epoch: 39\t100.00% complete. 87.00 seconds elapsed in epoch.\n",
      "Epoch: 39 \tTraining Loss: 1.3932 \tValidation Loss: 1.6411\n",
      "\t\tTraining Accuracy: 65.20%\t Validation Accuracy: 60.10%\n",
      "Epoch: 40\t100.00% complete. 86.61 seconds elapsed in epoch.\n",
      "Epoch: 40 \tTraining Loss: 1.3488 \tValidation Loss: 1.6396\n",
      "\t\tTraining Accuracy: 65.08%\t Validation Accuracy: 58.81%\n",
      "Epoch: 41\t92.19% complete. 79.98 seconds elapsed in epoch.\r"
     ]
    }
   ],
   "source": [
    "#Here the model is automatically saved by torch.save() in train function (extension of the saved file would be .pt)\n",
    "#Save_checkpoint method is for saving additional info like model classifier, num of epoch etc\n",
    "\n",
    "model, history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['valid'],\n",
    "    save_file_name=save_file_name, #This will save as .pt file\n",
    "    max_epochs_stop=25,\n",
    "    n_epochs=100,\n",
    "    print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer = load_checkpoint('models/model1.pth') # This will save model as .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68.75, 85.9375]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testiter = iter(dataloaders['test'])\n",
    "# Get a batch of testing images and labels\n",
    "features, targets = next(testiter)\n",
    "\n",
    "if train_on_gpu:\n",
    "    accuracy(model(features.to('cuda')), targets, topk=(1, 5))\n",
    "else:\n",
    "    accuracy(model(features), targets, topk=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint(model, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.000001, momentum=0.99, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training from Scratch.\n",
      "\n",
      "Epoch: 0\t100.00% complete. 86.55 seconds elapsed in epoch.\n",
      "Epoch: 0 \tTraining Loss: 1.2726 \tValidation Loss: 1.5766\n",
      "\t\tTraining Accuracy: 67.80%\t Validation Accuracy: 60.62%\n",
      "Epoch: 1\t100.00% complete. 86.56 seconds elapsed in epoch.\n",
      "Epoch: 1 \tTraining Loss: 1.2881 \tValidation Loss: 1.5887\n",
      "\t\tTraining Accuracy: 67.39%\t Validation Accuracy: 60.41%\n",
      "Epoch: 2\t100.00% complete. 86.44 seconds elapsed in epoch.\n",
      "Epoch: 2 \tTraining Loss: 1.2624 \tValidation Loss: 1.5740\n",
      "\t\tTraining Accuracy: 67.78%\t Validation Accuracy: 60.41%\n",
      "Epoch: 3\t100.00% complete. 87.01 seconds elapsed in epoch.\n",
      "Epoch: 3 \tTraining Loss: 1.3139 \tValidation Loss: 1.5848\n",
      "\t\tTraining Accuracy: 66.72%\t Validation Accuracy: 60.02%\n",
      "Epoch: 4\t100.00% complete. 86.72 seconds elapsed in epoch.\n",
      "Epoch: 4 \tTraining Loss: 1.3068 \tValidation Loss: 1.5755\n",
      "\t\tTraining Accuracy: 67.07%\t Validation Accuracy: 60.45%\n",
      "Epoch: 5\t100.00% complete. 86.52 seconds elapsed in epoch.\n",
      "Epoch: 5 \tTraining Loss: 1.2923 \tValidation Loss: 1.5756\n",
      "\t\tTraining Accuracy: 67.53%\t Validation Accuracy: 60.84%\n",
      "Epoch: 6\t100.00% complete. 86.52 seconds elapsed in epoch.\n",
      "Epoch: 6 \tTraining Loss: 1.2530 \tValidation Loss: 1.5688\n",
      "\t\tTraining Accuracy: 68.23%\t Validation Accuracy: 60.88%\n",
      "Epoch: 7\t100.00% complete. 86.74 seconds elapsed in epoch.\n",
      "Epoch: 7 \tTraining Loss: 1.3303 \tValidation Loss: 1.5652\n",
      "\t\tTraining Accuracy: 66.47%\t Validation Accuracy: 60.88%\n",
      "Epoch: 8\t100.00% complete. 86.45 seconds elapsed in epoch.\n",
      "Epoch: 8 \tTraining Loss: 1.2998 \tValidation Loss: 1.5669\n",
      "\t\tTraining Accuracy: 66.54%\t Validation Accuracy: 60.93%\n",
      "Epoch: 9\t100.00% complete. 86.26 seconds elapsed in epoch.\n",
      "Epoch: 9 \tTraining Loss: 1.2541 \tValidation Loss: 1.5764\n",
      "\t\tTraining Accuracy: 67.41%\t Validation Accuracy: 60.19%\n",
      "Epoch: 10\t100.00% complete. 86.44 seconds elapsed in epoch.\n",
      "Epoch: 10 \tTraining Loss: 1.3318 \tValidation Loss: 1.5715\n",
      "\t\tTraining Accuracy: 66.68%\t Validation Accuracy: 60.62%\n",
      "Epoch: 11\t100.00% complete. 86.65 seconds elapsed in epoch.\n",
      "Epoch: 11 \tTraining Loss: 1.2647 \tValidation Loss: 1.5574\n",
      "\t\tTraining Accuracy: 67.66%\t Validation Accuracy: 60.93%\n",
      "Epoch: 12\t100.00% complete. 86.79 seconds elapsed in epoch.\n",
      "Epoch: 12 \tTraining Loss: 1.3140 \tValidation Loss: 1.5868\n",
      "\t\tTraining Accuracy: 67.20%\t Validation Accuracy: 60.10%\n",
      "Epoch: 13\t100.00% complete. 86.20 seconds elapsed in epoch.\n",
      "Epoch: 13 \tTraining Loss: 1.2412 \tValidation Loss: 1.5697\n",
      "\t\tTraining Accuracy: 68.39%\t Validation Accuracy: 60.67%\n",
      "Epoch: 14\t100.00% complete. 86.34 seconds elapsed in epoch.\n",
      "Epoch: 14 \tTraining Loss: 1.2756 \tValidation Loss: 1.5632\n",
      "\t\tTraining Accuracy: 67.26%\t Validation Accuracy: 60.67%\n",
      "Epoch: 15\t100.00% complete. 85.95 seconds elapsed in epoch.\n",
      "Epoch: 15 \tTraining Loss: 1.2773 \tValidation Loss: 1.5703\n",
      "\t\tTraining Accuracy: 67.14%\t Validation Accuracy: 60.71%\n",
      "Epoch: 16\t100.00% complete. 86.39 seconds elapsed in epoch.\n",
      "Epoch: 16 \tTraining Loss: 1.2766 \tValidation Loss: 1.5619\n",
      "\t\tTraining Accuracy: 67.52%\t Validation Accuracy: 60.49%\n",
      "Epoch: 17\t100.00% complete. 86.56 seconds elapsed in epoch.\n",
      "Epoch: 17 \tTraining Loss: 1.2674 \tValidation Loss: 1.5696\n",
      "\t\tTraining Accuracy: 67.61%\t Validation Accuracy: 60.71%\n",
      "Epoch: 18\t100.00% complete. 86.22 seconds elapsed in epoch.\n",
      "Epoch: 18 \tTraining Loss: 1.2940 \tValidation Loss: 1.5701\n",
      "\t\tTraining Accuracy: 67.59%\t Validation Accuracy: 60.15%\n",
      "Epoch: 19\t100.00% complete. 86.41 seconds elapsed in epoch.\n",
      "Epoch: 19 \tTraining Loss: 1.3192 \tValidation Loss: 1.5754\n",
      "\t\tTraining Accuracy: 66.80%\t Validation Accuracy: 60.10%\n",
      "Epoch: 20\t100.00% complete. 86.45 seconds elapsed in epoch.\n",
      "Epoch: 20 \tTraining Loss: 1.2752 \tValidation Loss: 1.5677\n",
      "\t\tTraining Accuracy: 67.95%\t Validation Accuracy: 60.62%\n",
      "Epoch: 21\t100.00% complete. 86.05 seconds elapsed in epoch.\n",
      "Epoch: 21 \tTraining Loss: 1.2558 \tValidation Loss: 1.5720\n",
      "\t\tTraining Accuracy: 68.47%\t Validation Accuracy: 60.28%\n",
      "Epoch: 22\t100.00% complete. 86.47 seconds elapsed in epoch.\n",
      "Epoch: 22 \tTraining Loss: 1.3102 \tValidation Loss: 1.5554\n",
      "\t\tTraining Accuracy: 67.13%\t Validation Accuracy: 60.93%\n",
      "Epoch: 23\t100.00% complete. 85.83 seconds elapsed in epoch.\n",
      "Epoch: 23 \tTraining Loss: 1.2841 \tValidation Loss: 1.5431\n",
      "\t\tTraining Accuracy: 67.58%\t Validation Accuracy: 60.71%\n",
      "Epoch: 24\t100.00% complete. 86.30 seconds elapsed in epoch.\n",
      "Epoch: 24 \tTraining Loss: 1.2556 \tValidation Loss: 1.5612\n",
      "\t\tTraining Accuracy: 68.49%\t Validation Accuracy: 60.28%\n",
      "Epoch: 25\t100.00% complete. 86.51 seconds elapsed in epoch.\n",
      "Epoch: 25 \tTraining Loss: 1.2642 \tValidation Loss: 1.5596\n",
      "\t\tTraining Accuracy: 68.05%\t Validation Accuracy: 60.88%\n",
      "Epoch: 26\t100.00% complete. 86.83 seconds elapsed in epoch.\n",
      "Epoch: 26 \tTraining Loss: 1.2720 \tValidation Loss: 1.5529\n",
      "\t\tTraining Accuracy: 68.03%\t Validation Accuracy: 60.71%\n",
      "Epoch: 27\t100.00% complete. 86.32 seconds elapsed in epoch.\n",
      "Epoch: 27 \tTraining Loss: 1.2626 \tValidation Loss: 1.5748\n",
      "\t\tTraining Accuracy: 67.97%\t Validation Accuracy: 59.93%\n",
      "Epoch: 28\t100.00% complete. 86.48 seconds elapsed in epoch.\n",
      "Epoch: 28 \tTraining Loss: 1.2873 \tValidation Loss: 1.5590\n",
      "\t\tTraining Accuracy: 67.31%\t Validation Accuracy: 60.80%\n",
      "Epoch: 29\t100.00% complete. 86.60 seconds elapsed in epoch.\n",
      "Epoch: 29 \tTraining Loss: 1.2819 \tValidation Loss: 1.5674\n",
      "\t\tTraining Accuracy: 67.66%\t Validation Accuracy: 60.36%\n",
      "Epoch: 30\t100.00% complete. 86.96 seconds elapsed in epoch.\n",
      "Epoch: 30 \tTraining Loss: 1.2868 \tValidation Loss: 1.5651\n",
      "\t\tTraining Accuracy: 67.66%\t Validation Accuracy: 60.32%\n",
      "Epoch: 31\t100.00% complete. 86.82 seconds elapsed in epoch.\n",
      "Epoch: 31 \tTraining Loss: 1.2784 \tValidation Loss: 1.5504\n",
      "\t\tTraining Accuracy: 67.64%\t Validation Accuracy: 60.84%\n",
      "Epoch: 32\t100.00% complete. 86.31 seconds elapsed in epoch.\n",
      "Epoch: 32 \tTraining Loss: 1.2815 \tValidation Loss: 1.5487\n",
      "\t\tTraining Accuracy: 67.97%\t Validation Accuracy: 60.80%\n",
      "Epoch: 33\t100.00% complete. 86.31 seconds elapsed in epoch.\n",
      "Epoch: 33 \tTraining Loss: 1.2867 \tValidation Loss: 1.5623\n",
      "\t\tTraining Accuracy: 67.80%\t Validation Accuracy: 60.88%\n",
      "Epoch: 34\t100.00% complete. 86.29 seconds elapsed in epoch.\n",
      "Epoch: 34 \tTraining Loss: 1.2907 \tValidation Loss: 1.5567\n",
      "\t\tTraining Accuracy: 67.11%\t Validation Accuracy: 60.88%\n",
      "Epoch: 35\t100.00% complete. 86.62 seconds elapsed in epoch.\n",
      "Epoch: 35 \tTraining Loss: 1.2605 \tValidation Loss: 1.5615\n",
      "\t\tTraining Accuracy: 68.11%\t Validation Accuracy: 60.84%\n",
      "Epoch: 36\t100.00% complete. 86.63 seconds elapsed in epoch.\n",
      "Epoch: 36 \tTraining Loss: 1.2859 \tValidation Loss: 1.5737\n",
      "\t\tTraining Accuracy: 66.83%\t Validation Accuracy: 60.02%\n",
      "Epoch: 37\t100.00% complete. 86.14 seconds elapsed in epoch.\n",
      "Epoch: 37 \tTraining Loss: 1.2466 \tValidation Loss: 1.5586\n",
      "\t\tTraining Accuracy: 68.44%\t Validation Accuracy: 60.49%\n",
      "Epoch: 38\t100.00% complete. 86.16 seconds elapsed in epoch.\n",
      "Epoch: 38 \tTraining Loss: 1.2679 \tValidation Loss: 1.5604\n",
      "\t\tTraining Accuracy: 67.96%\t Validation Accuracy: 60.58%\n",
      "Epoch: 39\t100.00% complete. 86.12 seconds elapsed in epoch.\n",
      "Epoch: 39 \tTraining Loss: 1.2601 \tValidation Loss: 1.5569\n",
      "\t\tTraining Accuracy: 67.68%\t Validation Accuracy: 60.10%\n",
      "Epoch: 40\t100.00% complete. 86.44 seconds elapsed in epoch.\n",
      "Epoch: 40 \tTraining Loss: 1.2922 \tValidation Loss: 1.5555\n",
      "\t\tTraining Accuracy: 67.17%\t Validation Accuracy: 60.67%\n",
      "Epoch: 41\t100.00% complete. 86.39 seconds elapsed in epoch.\n",
      "Epoch: 41 \tTraining Loss: 1.2776 \tValidation Loss: 1.5663\n",
      "\t\tTraining Accuracy: 67.44%\t Validation Accuracy: 60.32%\n",
      "Epoch: 42\t100.00% complete. 85.64 seconds elapsed in epoch.\n",
      "Epoch: 42 \tTraining Loss: 1.2377 \tValidation Loss: 1.5568\n",
      "\t\tTraining Accuracy: 68.33%\t Validation Accuracy: 60.45%\n",
      "Epoch: 43\t3.12% complete. 2.72 seconds elapsed in epoch.\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9c7f51d00f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_epochs_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     print_every=1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-4d975830764e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, save_file_name, max_epochs_stop, n_epochs, print_every)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Tensors to gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_on_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \"\"\"\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['valid'],\n",
    "    save_file_name=save_file_name, #This will save as .pt file\n",
    "    max_epochs_stop=25,\n",
    "    n_epochs=60,\n",
    "    print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been trained for: 44 epochs.\n",
      "\n",
      "Epoch: 0\t100.00% complete. 86.39 seconds elapsed in epoch.\n",
      "Epoch: 0 \tTraining Loss: 1.2777 \tValidation Loss: 1.5675\n",
      "\t\tTraining Accuracy: 67.48%\t Validation Accuracy: 60.36%\n",
      "Epoch: 1\t100.00% complete. 86.27 seconds elapsed in epoch.\n",
      "Epoch: 1 \tTraining Loss: 1.2365 \tValidation Loss: 1.5526\n",
      "\t\tTraining Accuracy: 68.44%\t Validation Accuracy: 60.75%\n",
      "\n",
      "Best epoch: 1 with loss: 1.55 and acc: 60.75%\n",
      "216.04 total seconds elapsed. 216.04 seconds per epoch.\n"
     ]
    }
   ],
   "source": [
    "model, history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['valid'],\n",
    "    save_file_name=save_file_name, #This will save as .pt file\n",
    "    max_epochs_stop=25,\n",
    "    n_epochs=2,\n",
    "    print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
